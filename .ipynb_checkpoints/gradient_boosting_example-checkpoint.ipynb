{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Example\n",
    "\n",
    "This notebook demonstrates the effectiveness of our custom Gradient Boosting implementation for both regression and classification tasks, comparing it with simpler models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from gradient_boosting import GradientBoostingRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=1, noise=0.4, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "lr_mse = mean_squared_error(y_test, lr_pred)\n",
    "\n",
    "# Train and evaluate GradientBoostingRegressor\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "gb_pred = gb_reg.predict(X_test)\n",
    "gb_mse = mean_squared_error(y_test, gb_pred)\n",
    "\n",
    "print(f\"Linear Regression MSE: {lr_mse:.4f}\")\n",
    "print(f\"Gradient Boosting MSE: {gb_mse:.4f}\")\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, alpha=0.5, label='True values')\n",
    "plt.plot(X_test, lr_pred, color='red', label='Linear Regression')\n",
    "plt.plot(X_test, gb_pred, color='green', label='Gradient Boosting')\n",
    "plt.legend()\n",
    "plt.title('Regression: True values vs Predictions')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Train and evaluate GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = (gb_clf.predict_proba(X_test) > 0.5).astype(int)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.4f}\")\n",
    "\n",
    "# Visualize the decision boundaries\n",
    "def plot_decision_boundary(clf, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(dt, X, y, 'Decision Tree')\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(gb_clf, X, y, 'Gradient Boosting')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the effectiveness of our custom Gradient Boosting implementation for both regression and classification tasks. \n",
    "\n",
    "For the regression task, we can see that Gradient Boosting outperforms Linear Regression in terms of Mean Squared Error. The visualization shows how Gradient Boosting can capture non-linear patterns in the data that Linear Regression misses.\n",
    "\n",
    "For the classification task, Gradient Boosting achieves higher accuracy compared to a simple Decision Tree. The decision boundary plots illustrate how Gradient Boosting creates a more complex and potentially more accurate decision boundary compared to the Decision Tree.\n",
    "\n",
    "These results highlight the power of Gradient Boosting in handling both regression and classification problems, often outperforming simpler models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}