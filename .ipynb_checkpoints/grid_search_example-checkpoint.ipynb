{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Example\n",
    "\n",
    "This notebook demonstrates the usage of our custom Grid Search implementation for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from grid_search import GridSearchCV\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "# Create and run the grid search\n",
    "svc = SVC()\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search_heatmap(grid_search, param1, param2):\n",
    "    # Extract scores and parameters\n",
    "    scores = np.array([result['mean_test_score'] for result in grid_search.cv_results_])\n",
    "    params1 = grid_search.param_grid[param1]\n",
    "    params2 = grid_search.param_grid[param2]\n",
    "    \n",
    "    # Create a 2D array to hold the scores\n",
    "    score_grid = np.full((len(params1), len(params2)), np.nan)\n",
    "    \n",
    "    # Fill in the score grid\n",
    "    for idx, result in enumerate(grid_search.cv_results_):\n",
    "        i = params1.index(result['params'][param1])\n",
    "        j = params2.index(result['params'][param2])\n",
    "        score_grid[i, j] = result['mean_test_score']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(score_grid, interpolation='nearest', cmap='viridis')\n",
    "    plt.title(f'Grid Search Heatmap ({param1} vs {param2})')\n",
    "    plt.xlabel(param2)\n",
    "    plt.ylabel(param1)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.xticks(np.arange(len(params2)), params2)\n",
    "    plt.yticks(np.arange(len(params1)), params1)\n",
    "    \n",
    "    for i, j in product(range(len(params1)), range(len(params2))):\n",
    "        if not np.isnan(score_grid[i, j]):\n",
    "            plt.text(j, i, f\"{score_grid[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot heatmaps for different parameter combinations\n",
    "plot_grid_search_heatmap(grid_search, 'C', 'gamma')\n",
    "plot_grid_search_heatmap(grid_search, 'C', 'kernel')\n",
    "plot_grid_search_heatmap(grid_search, 'kernel', 'gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the usage of our custom Grid Search implementation for hyperparameter tuning. We used it to find the best parameters for an SVM classifier and visualized the results using heatmaps. This approach helps in understanding the impact of different hyperparameters on model performance and in selecting the optimal combination."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}